{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11534596,"sourceType":"datasetVersion","datasetId":7234372},{"sourceId":11552235,"sourceType":"datasetVersion","datasetId":7135994},{"sourceId":11552473,"sourceType":"datasetVersion","datasetId":7244390},{"sourceId":11552506,"sourceType":"datasetVersion","datasetId":7244410},{"sourceId":11552552,"sourceType":"datasetVersion","datasetId":7135930}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# E-commerce recommendation system\nWe will be using [Surprise](https://surprise.readthedocs.io/en/stable/index.html), a Python scikit for recommender systems, to build our model. Surprise contains lots of algorithms that are commonly used for recommendation.","metadata":{}},{"cell_type":"code","source":"!pip install surprise\n\n## if getting a numpy related error, revert back to an older version of numpy\n#!pip install numpy==1.23.5","metadata":{"id":"WadMQUDi6Dgw","outputId":"743685aa-8a8b-4d30-8c08-827e5d58b6d7","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T23:11:47.516267Z","iopub.execute_input":"2025-05-09T23:11:47.516752Z","iopub.status.idle":"2025-05-09T23:11:51.467631Z","shell.execute_reply.started":"2025-05-09T23:11:47.516708Z","shell.execute_reply":"2025-05-09T23:11:51.466025Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After installing Surprise, we can begin to create our model. We are using the Amazon Reviews '23 dataset found [here](https://amazon-reviews-2023.github.io/data_processing/5core.html). As our memory is limited and the datasets are quite large with many of them containing miillions of reviews, we have been limited to smaller datasets from here. Specifically, we will be using the \"all beauty\" review data.\n\nWe can read the data, which is downloaded as a csv file into a pandas dataframe. The dataframe is then trimmed to only include the columns with the user ID, item ID, and the rating so that it can be converted into a Surprise dataset.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom surprise import Dataset, Reader, SVD, SVDpp, CoClustering, KNNBasic\nfrom surprise.model_selection import cross_validate, train_test_split\nfrom surprise import accuracy\nfrom surprise.model_selection import KFold, GridSearchCV\n\nfilename =\"/kaggle/input/all-beauty/All_Beauty.csv\"\n\ndf = pd.read_csv(filename)\n\nreader = Reader(rating_scale=(1, 5))\n\ndata = Dataset.load_from_df(df[[\"user_id\", \"parent_asin\", \"rating\"]], reader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T23:11:51.470332Z","iopub.execute_input":"2025-05-09T23:11:51.470707Z","iopub.status.idle":"2025-05-09T23:11:51.507221Z","shell.execute_reply.started":"2025-05-09T23:11:51.470661Z","shell.execute_reply":"2025-05-09T23:11:51.506033Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we want to return the top-N recommendations for each user. We will try multiple different models.","metadata":{}},{"cell_type":"code","source":"# This function gets the top n (5 by default) recommendations for a user (from documentation)\ndef get_top_n(predictions, n=5):\n    \"\"\"\n    Args:\n        predictions(list of Prediction objects): The list of predictions, as\n            returned by the test method of an algorithm.\n        n(int): The number of recommendation to output for each user.\n\n    Returns:\n    A dict where keys are user (raw) ids and values are lists of tuples:\n        [(raw item id, rating estimation), ...] of size n.\n    \"\"\"\n\n    # Map the predictions to each user\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T23:11:51.508419Z","iopub.execute_input":"2025-05-09T23:11:51.508702Z","iopub.status.idle":"2025-05-09T23:11:51.517064Z","shell.execute_reply.started":"2025-05-09T23:11:51.508680Z","shell.execute_reply":"2025-05-09T23:11:51.515536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Precision@K and Recall@K from documentation\ndef precision_recall_at_k(predictions, k=10, threshold=3.5):\n    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n\n    # First map the predictions to each user.\n    user_est_true = defaultdict(list)\n    for uid, _, true_r, est, _ in predictions:\n        user_est_true[uid].append((est, true_r))\n\n    precisions = dict()\n    recalls = dict()\n    for uid, user_ratings in user_est_true.items():\n\n        # Sort user ratings by estimated value\n        user_ratings.sort(key=lambda x: x[0], reverse=True)\n\n        # Number of relevant items\n        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n\n        # Number of recommended items in top k\n        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n\n        # Number of relevant and recommended items in top k\n        n_rel_and_rec_k = sum(\n            ((true_r >= threshold) and (est >= threshold))\n            for (est, true_r) in user_ratings[:k]\n        )\n\n        # Precision@K: Proportion of recommended items that are relevant\n        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n\n        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n\n        # Recall@K: Proportion of relevant items that are recommended\n        # When n_rel is 0, Recall is undefined. We here set it to 0.\n\n        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n\n    return precisions, recalls\n\nkf = KFold(n_splits=5)\n\n## calculates the average precision@k and recall@k\ndef get_precision_recall(prediction):\n    precision_list = []\n    recall_list = []\n    for trainset, testset in kf.split(data):\n        precisions, recalls = precision_recall_at_k(prediction, k=5, threshold=4)\n    \n        # Precision and recall can then be averaged over all users\n        precision_list.append(sum(prec for prec in precisions.values()) / len(precisions))\n        recall_list.append(sum(rec for rec in recalls.values()) / len(recalls))\n    print(\"avg precision: \" + str(np.mean(precision_list)))\n    print(\"avg recall: \" + str(np.mean(recall_list)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T23:11:51.518749Z","iopub.execute_input":"2025-05-09T23:11:51.519307Z","iopub.status.idle":"2025-05-09T23:11:51.543320Z","shell.execute_reply.started":"2025-05-09T23:11:51.519267Z","shell.execute_reply":"2025-05-09T23:11:51.541747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Create trainset and testset using 80/20 split\ntrainset, testset = train_test_split(data, test_size=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T23:11:51.546941Z","iopub.execute_input":"2025-05-09T23:11:51.548052Z","iopub.status.idle":"2025-05-09T23:11:51.576517Z","shell.execute_reply.started":"2025-05-09T23:11:51.548000Z","shell.execute_reply":"2025-05-09T23:11:51.575337Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The first model we will try is SVD.","metadata":{}},{"cell_type":"code","source":"param_grid_svd = {\n    'n_epochs': [20, 25, 30],\n    'n_factors': [140, 155, 160],\n    'lr_all': [0.017, 0.02, 0.025],\n    'reg_all': [0.12, 0.15, 0.18], \n    'init_std_dev': [0.05, 0.1, 0.15],\n}\n\n# Initialize GridSearchCV with the SVD algorithm\ngs_svd = GridSearchCV(SVD, param_grid_svd, measures=['rmse', 'mae'], cv=5, n_jobs=-1)\n\n# Perform grid search on the entire dataset\ngs_svd.fit(data)\n\n# Get the best score and corresponding parameters\nprint(\"Best RMSE score:\", gs_svd.best_score['rmse'])\nprint(\"Best parameters:\", gs_svd.best_params['rmse'])\n\n# Use the best parameters to train the final model\nbest_params = gs_svd.best_params['rmse']\nsvd_algo = SVD(n_factors=best_params['n_factors'], lr_all=best_params['lr_all'], reg_all=best_params['reg_all'], \n              n_epochs=best_params['n_epochs'], init_std_dev=best_params['init_std_dev'])\n\n# Train the model on the trainset\nsvd_algo.fit(trainset)\n\n# Test the model on the testset\nsvd_predictions = svd_algo.test(testset)\naccuracy.rmse(svd_predictions)","metadata":{"id":"giMFGRxbgxaN","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T23:11:51.577366Z","iopub.execute_input":"2025-05-09T23:11:51.577721Z","iopub.status.idle":"2025-05-09T23:12:20.953576Z","shell.execute_reply.started":"2025-05-09T23:11:51.577678Z","shell.execute_reply":"2025-05-09T23:12:20.952466Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Cross validation for SVD:","metadata":{}},{"cell_type":"code","source":"cross_validate(svd_algo, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True)\n\nget_precision_recall(svd_predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T23:12:20.955017Z","iopub.execute_input":"2025-05-09T23:12:20.955404Z","iopub.status.idle":"2025-05-09T23:12:21.192052Z","shell.execute_reply.started":"2025-05-09T23:12:20.955365Z","shell.execute_reply":"2025-05-09T23:12:21.191169Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we will try an extension of SVD, using the SVD++ algorithm.","metadata":{}},{"cell_type":"code","source":"#IMPORTANT NOTES\n\n#When we initially ran this algorithm, we obsered low scores for RMSE and MAE (0.40 and 0.31 respectively).\n#However, we were unable to reproduce these results consistently in later runs, even when using the same dataset and training/testing sets\n#We investigated the issue by running the same code on multiple environments such as Google Colab and locally, and trying multiple random seeds and tuning hyperparameters\n#Despite our efforts, we could not replicate the original results, and we believe this could be due to cached variables in the kernel or unintentional overlap in train-test sets\n#Moving forward, we continued to tune this algorithm in the best possible way using cross-validation and tuning hyperparameters, so these are the new and correct results.\n\n\n# Define parameter grid for tuning\nparam_grid_svdpp = {\n    'n_epochs': [35, 40, 45],\n    'lr_all': [0.004, 0.005, 0.006],\n    'reg_all': [0.08, 0.1, 0.12]\n}\n\n# Run grid search\ngs_svdpp = GridSearchCV(SVDpp, param_grid_svdpp, measures=['rmse', 'mae'], cv=3, joblib_verbose=1)\ngs_svdpp.fit(data)\n\n# Prints the best parameters to show which ones will be used to train final model\nprint(\"Best RMSE score:\", gs_svdpp.best_score['rmse'])\nprint(\"Best parameters:\", gs_svdpp.best_params['rmse'])\n\n# Train final model with best parameters\nsvdpp_algo = gs_svdpp.best_estimator['rmse']\nsvdpp_algo.fit(trainset)\n\n# This Evaluates the test set \nsvdpp_predictions = svdpp_algo.test(testset)\nprint(\"Final RMSE:\", accuracy.rmse(svdpp_predictions))\nprint(\"Final MAE:\", accuracy.mae(svdpp_predictions))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T23:12:21.193387Z","iopub.execute_input":"2025-05-09T23:12:21.193622Z","iopub.status.idle":"2025-05-09T23:12:29.901090Z","shell.execute_reply.started":"2025-05-09T23:12:21.193601Z","shell.execute_reply":"2025-05-09T23:12:29.899361Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Cross validation for SVD++","metadata":{}},{"cell_type":"code","source":"cross_validate(svdpp_algo, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True)\n\nget_precision_recall(svdpp_predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T23:13:43.141363Z","iopub.execute_input":"2025-05-09T23:13:43.141753Z","iopub.status.idle":"2025-05-09T23:13:43.755480Z","shell.execute_reply.started":"2025-05-09T23:13:43.141724Z","shell.execute_reply":"2025-05-09T23:13:43.754497Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next we'll try coclustering.","metadata":{}},{"cell_type":"code","source":"param_grid_cocluster = {\n    'n_epochs': [11, 13, 15],\n    'n_cltr_u':[10, 12, 15],\n    'n_cltr_i':[5, 7, 10]\n}\n\n# Initialize GridSearchCV with the CoClustering algorithm\ngs_cc = GridSearchCV(CoClustering, param_grid_cocluster, measures=['rmse', 'mae'], cv=3, n_jobs=-1)\n\n# Perform grid search on the entire dataset\ngs_cc.fit(data)\n\n# Get the best score and corresponding parameters\nprint(\"Best RMSE score:\", gs_cc.best_score['rmse'])\nprint(\"Best parameters:\", gs_cc.best_params['rmse'])\n\n# Use the best parameters to train the final model\nbest_params = gs_cc.best_params['rmse']\ncc_algo = CoClustering(n_epochs=best_params['n_epochs'], n_cltr_u=best_params['n_cltr_u'], \n                      n_cltr_i=best_params['n_cltr_i'])\n\ncc_algo.fit(trainset)\n\n# Test the model on the testset\ncc_predictions = cc_algo.test(testset)\n\naccuracy.rmse(cc_predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T23:12:29.902251Z","iopub.execute_input":"2025-05-09T23:12:29.902695Z","iopub.status.idle":"2025-05-09T23:12:32.213973Z","shell.execute_reply.started":"2025-05-09T23:12:29.902661Z","shell.execute_reply":"2025-05-09T23:12:32.213135Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Get the accuracy measures for coclustering.","metadata":{}},{"cell_type":"code","source":"cross_validate(cc_algo, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True)\n\nget_precision_recall(cc_predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T23:12:32.215183Z","iopub.execute_input":"2025-05-09T23:12:32.215496Z","iopub.status.idle":"2025-05-09T23:12:32.536970Z","shell.execute_reply.started":"2025-05-09T23:12:32.215463Z","shell.execute_reply":"2025-05-09T23:12:32.536059Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next we'll try KNN.","metadata":{}},{"cell_type":"code","source":"param_grid_knn = {\n    'k': [30, 40, 45],\n    'min_k': [5, 7, 9]\n}\nsim_options = {'name': 'cosine', 'user_based': False}\n\n# Initialize GridSearchCV with the CoClustering algorithm\ngs_knn = GridSearchCV(KNNBasic, param_grid_knn, measures=['rmse', 'mae'], cv=5, n_jobs=-1)\n\n# Perform grid search on the entire dataset\ngs_knn.fit(data)\n\n# Get the best score and corresponding parameters\nprint(\"Best RMSE score:\", gs_knn.best_score['rmse'])\nprint(\"Best parameters:\", gs_knn.best_params['rmse'])\n\n# Use the best parameters to train the final model\nbest_params = gs_knn.best_params['rmse']\nknn_algo = KNNBasic(k=best_params['k'], min_k=best_params['min_k'], \n                      sim_options=sim_options)\n\nknn_algo.fit(trainset)\n\n# Test the model on the testset\nknn_predictions = knn_algo.test(testset)\naccuracy.rmse(knn_predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T23:12:32.538025Z","iopub.execute_input":"2025-05-09T23:12:32.538310Z","iopub.status.idle":"2025-05-09T23:12:32.782964Z","shell.execute_reply.started":"2025-05-09T23:12:32.538279Z","shell.execute_reply":"2025-05-09T23:12:32.782011Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Get accuracy measures for KNN.","metadata":{}},{"cell_type":"code","source":"cross_validate(knn_algo, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True)\n\nget_precision_recall(knn_predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T23:12:32.784225Z","iopub.execute_input":"2025-05-09T23:12:32.784520Z","iopub.status.idle":"2025-05-09T23:12:32.871865Z","shell.execute_reply.started":"2025-05-09T23:12:32.784499Z","shell.execute_reply":"2025-05-09T23:12:32.870801Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since SVD and SVD++ have similar performance but SVD runs faster, we will apply the SVD model to get the top 5 items.","metadata":{}},{"cell_type":"code","source":"full_trainset = data.build_full_trainset()\nnew_testset = full_trainset.build_anti_testset() # get all items not yet rated by user\nnew_svd_predictions = svd_algo.test(new_testset)\nsvd_top_n = get_top_n(new_svd_predictions, n=5)\n\nfirst_five_items = list(svd_top_n.items())[:5]\nfor key, value in first_five_items:\n    # Generate a link using the format: https://www.amazon.com/dp/[ASIN]\n    print(key, [f\"https://www.amazon.com/dp/{iid}\" for (iid, _) in value])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T23:12:32.872934Z","iopub.execute_input":"2025-05-09T23:12:32.873352Z","iopub.status.idle":"2025-05-09T23:12:33.547607Z","shell.execute_reply.started":"2025-05-09T23:12:32.873322Z","shell.execute_reply":"2025-05-09T23:12:33.546665Z"}},"outputs":[],"execution_count":null}]}